{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a9dcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "working_dir = Path.cwd()\n",
    "while working_dir.name != 'CausalMTR-BC':\n",
    "    working_dir = working_dir.parent\n",
    "    if working_dir == Path.home():\n",
    "        raise FileNotFoundError(\"Base directory 'CausalMTR-BC' not found\")\n",
    "os.chdir(working_dir)\n",
    "\n",
    "import mtr\n",
    "from mtr.models.model import MotionTransformer\n",
    "from configs.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e30953b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MotionTransformer(\n",
       "  (context_encoder): MTREncoder(\n",
       "    (agent_polyline_encoder): PointNetPolylineEncoder(\n",
       "      (pre_mlps): Sequential(\n",
       "        (0): Linear(in_features=39, out_features=64, bias=False)\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (mlps): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=64, bias=False)\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "      (out_mlps): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (map_polyline_encoder): PointNetPolylineEncoder(\n",
       "      (pre_mlps): Sequential(\n",
       "        (0): Linear(in_features=13, out_features=64, bias=False)\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): ReLU(inplace=True)\n",
       "      )\n",
       "      (mlps): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=64, bias=False)\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Linear(in_features=64, out_features=64, bias=False)\n",
       "        (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "      (out_mlps): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (mask_predictor): AttnMaskPredictor(\n",
       "      (proj_to_mask_q): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (proj_to_mask_k): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (competition): Competition(\n",
       "        (attn_blocks): ModuleList(\n",
       "          (0-1): 2 x Block(\n",
       "            (attn): MultiheadAttentionLocal(\n",
       "              (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            )\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "            )\n",
       "            (norm_q): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_k): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_attn): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (sparsification): Bernoulli(\n",
       "        (normalization_func): Normalization()\n",
       "      )\n",
       "    )\n",
       "    (self_attn_layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttentionLocal(\n",
       "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (motion_decoder): MTRDecoder(\n",
       "    (in_proj_center_obj): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (in_proj_obj): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (obj_decoder_layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (ca_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (cross_attn): MultiheadAttentionMask(\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (in_proj_map): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (map_decoder_layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (ca_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (cross_attn): MultiheadAttentionLocal(\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (obj_pos_encoding_layer): Sequential(\n",
       "      (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (dense_future_head): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=256, bias=False)\n",
       "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Linear(in_features=256, out_features=280, bias=True)\n",
       "    )\n",
       "    (future_traj_mlps): Sequential(\n",
       "      (0): Linear(in_features=160, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (traj_fusion_mlps): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (intention_query_mlps): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (query_feature_fusion_layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=False)\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (motion_reg_heads): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Linear(in_features=256, out_features=280, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (motion_cls_heads): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "model_cfg = config.MODEL\n",
    "model_cfg.CONTEXT_ENCODER.NUM_INPUT_ATTR_AGENT = 38\n",
    "model_cfg.CONTEXT_ENCODER.NUM_ATTN_LAYERS = 1\n",
    "model_cfg.CONTEXT_ENCODER.NUM_CHANNEL_IN_MLP_AGENT = 64\n",
    "model_cfg.MOTION_DECODER.NUM_DECODER_LAYERS = 1\n",
    "model_cfg.MOTION_DECODER.NUM_FUTURE_FRAMES = 40\n",
    "\n",
    "MotionTransformer(config=model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import make_dataclass, asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d76ef6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = config.MODEL.CONTEXT_ENCODER.MASK_PREDICTOR.MODEL.competition_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da660cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from configs import BaseConfig\n",
    "isinstance(d, BaseConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3fd1c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['iters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e3c1a42",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'CompetitionConfig' object is not a mapping",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m {**d}\n",
      "\u001b[31mTypeError\u001b[39m: 'CompetitionConfig' object is not a mapping"
     ]
    }
   ],
   "source": [
    "{**d}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef89e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmtr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
